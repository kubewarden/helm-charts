name: Backup-Restore E2E Test

on:
  workflow_dispatch:
    inputs:
      destroy_runner:
        description: Destroy the auto-generated self-hosted runner
        default: true
        type: boolean
      k3s_version:
        description: K3S version to use
        type: string
        default: 'v1.31.7+k3s1'
      runner_template:
        description: Runner template to use
        default: kubewarden-e2e-ci-runner-x86-64-template-v1
        type: string
      zone:
        description: GCP zone to host the runner
        default: us-central1-f
        type: string
  schedule:
    # Every Friday at 4am UTC (11pm in us-central1)
    - cron: '0 4 * * 5'

jobs:
  create-runner:
    uses: ./.github/workflows/sub_create-runner.yaml
    secrets:
      credentials: ${{ secrets.GCP_CREDENTIALS }}
      pat_token: ${{ secrets.KUBEWARDEN_SELF_HOSTED_RUNNER_PAT_TOKEN }}
    with:
      runner_template: ${{ inputs.runner_template || 'kubewarden-e2e-ci-runner-x86-64-template-v1' }}
      zone: ${{ inputs.zone || 'us-central1-f' }}

  e2e:
    needs: create-runner
    if: ${{ always() }}
    runs-on: ${{ needs.create-runner.outputs.runner_label }}
    env:
      INSTALL_K3S_SKIP_ENABLE: true
      K3S_KUBECONFIG_MODE: 0644
      K3S_VERSION: ${{ inputs.k3s_version || 'v1.31.7+k3s1' }}
    steps:
      - name: Checkout
        id: checkout
        uses: actions/checkout@93cb6efe18208431cddfb8368fd83d5badbf9bfd # v5

      - name: Checkout e2e
        uses: actions/checkout@93cb6efe18208431cddfb8368fd83d5badbf9bfd # v5
        with:
          repository: ${{ github.repository_owner }}/kubewarden-end-to-end-tests
          path: e2e-tests
          submodules: 'true'

      # Fetching backup-restore operator because we cannot install from official chart repo yet
      # We have to wait for 9.0.0 release
      # https://github.com/kubewarden/helm-charts/pull/855
      - name: Checkout backup-restore 
        uses: actions/checkout@93cb6efe18208431cddfb8368fd83d5badbf9bfd # v5
        with:
          repository: rancher/backup-restore-operator
          path: backup-restore-operator

      - name: Authenticate to GCP
        id: authenticate
        uses: google-github-actions/auth@7c6bc770dae815cd3e89ee6cdf493a5fab2cc093 # v3
        with:
          credentials_json: ${{ secrets.GCP_CREDENTIALS }}

      - name: Setup gcloud
        id: setup_gcloud
        uses: google-github-actions/setup-gcloud@aa5489c8933f4cc7a4f7d45035b3b1440c9c10db # v3

      - name: Setup Go
        id: setup_go
        uses: actions/setup-go@44694675825211faa026b3c33043df3e48a5fa00 # v6
        with:
          cache-dependency-path: ./e2e-tests/tests/ginkgo-e2e/go.sum
          go-version-file: ./e2e-tests/tests/ginkgo-e2e/go.mod

      - name: Define needed system variables
        run: |
          # Add missing PATH, removed in recent distributions for security reasons...
          echo "/usr/local/bin" >> ${GITHUB_PATH}

      # Workaround, I need to take time to update the opensuse image we use in the CI
      - name: Update helm package
        run: sudo zypper --non-interactive update helm
      
      - name: Install K3S
        id: install_k3s
        run: |
          cd ${GITHUB_WORKSPACE}/e2e-tests/tests/ginkgo-e2e
          make e2e-install-k3s

      - name: Install backup-restore components
        id: install_backup_restore
        run: |
          # BUILD BACKUP_RESTORE OPERATOR FROM SOURCE REPO
          # WAITING ON 9.0.0 RELEASE TO INSTALL FROM OFFICIAL CHART REPO
          # ONCE RELEASED, WE WILL INSTALL FROM THE GINKGO CODE
          # https://github.com/kubewarden/helm-charts/pull/855
          cd ${GITHUB_WORKSPACE}/backup-restore-operator
          helm install --wait  --create-namespace -n cattle-resources-system  rancher-backup-crd ./charts/rancher-backup-crd
          sed -i 's/%TAG%/v9.0.0-rc.3/g' ./charts/rancher-backup/values.yaml
          helm install --wait -n cattle-resources-system rancher-backup ./charts/rancher-backup --set persistence.enabled=true --set persistence.storageClass=local-path --set optionalResources.kubewarden.enabled=true
      
      - name: Extract component versions/informations
        id: component
        run: |
          # Extract rancher-backup-operator version
          BACKUP_OPERATOR_VERSION=$(kubectl get pod \
                                     --namespace cattle-resources-system \
                                     -l app.kubernetes.io/name=rancher-backup \
                                     -o jsonpath={.items[*].status.containerStatuses[*].image} 2> /dev/null || true)

      - name: Install Kubewarden
        id: install_kubewarden
        run: |
          cd ${GITHUB_WORKSPACE}/e2e-tests/tests/ginkgo-e2e
          make e2e-install-kubewarden
          
          # Export values
          echo "audit_scanner_version=${AUDIT_SCANNER_VERSION}" | tee -a ${GITHUB_OUTPUT}
          echo "backup_operator_version=${BACKUP_OPERATOR_VERSION}" | tee -a ${GITHUB_OUTPUT}
          echo "kubewarden_controller_version=${KUBEWARDEN_CONTROLLER_VERSION}" | tee -a ${GITHUB_OUTPUT}
          echo "policy_server_version=${POLICY_SERVER_VERSION}" | tee -a  ${GITHUB_OUTPUT}
      # This step must be called in each worklow that wants a summary!
      - name: Add summary
        if: ${{ always() }}
        env: 
          STEP_TO_REPORT: backup-restore
        uses: ./.github/actions/logs-and-summary

      - name: Test Backup/Restore kubewarden resources
        id: test_backup_restore
        run: |
          cd ${GITHUB_WORKSPACE}/e2e-tests/tests/ginkgo-e2e
          make e2e-full-backup-restore

  clean-and-delete-runner:
    needs: [create-runner, e2e]
    if: ${{ always() && needs.create-runner.result == 'success' && (github.event_name == 'schedule' || inputs.destroy_runner == true) }}
    uses: ./.github/workflows/sub_clean-and-delete-runner.yaml
    secrets:
      credentials: ${{ secrets.GCP_CREDENTIALS }}
      pat_token: ${{ secrets.KUBEWARDEN_SELF_HOSTED_RUNNER_PAT_TOKEN }}
    with:
      create_runner_result: ${{ needs.create-runner.result }}
      destroy_runner: ${{ inputs.destroy_runner || true }}
      runner_hostname: ${{ needs.create-runner.outputs.runner_hostname }}
      runner_label: ${{ needs.create-runner.outputs.runner_label }}
      zone: ${{ inputs.zone || 'us-central1-f' }}
